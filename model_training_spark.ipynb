{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "HW2 Starter Code\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html\n",
    "\n",
    "https://stackoverflow.com/questions/38839924/how-to-combine-n-grams-into-one-vocabulary-in-spark\n",
    "\n",
    "https://nlp.johnsnowlabs.com/docs/en/transformers#bertsentenceembeddings\n",
    "\n",
    "https://nlp.johnsnowlabs.com/docs/en/annotators\n",
    "\n",
    "https://nlp.johnsnowlabs.com/docs/en/training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mefeye9-4ZCd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "_b8699zD4ZCe"
   },
   "outputs": [],
   "source": [
    "path_name = 'gs://eecs6893-hw2-cluster/final-project/training/sentiment140.csv'\n",
    "df = pd.read_csv(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "wXZRZExh-UrO",
    "outputId": "50ba1e13-f4a3-4b65-93f1-abf22a79916f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"i'm 10x cooler than all of you! \"</td>\n",
       "      <td>4</td>\n",
       "      <td>im 10x cooler than all of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b'O.kk? Thats weird I cant stop following peop...</td>\n",
       "      <td>0</td>\n",
       "      <td>o kk thats weird i cant stop following people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b'what a beautiful day not to got to my first ...</td>\n",
       "      <td>4</td>\n",
       "      <td>what a beautiful day not to got to my first class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>b\".@HildyGottlieb &amp;amp; I was just saying to M...</td>\n",
       "      <td>4</td>\n",
       "      <td>and i was just saying to mahaal yesterday ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>b'kinda sad and confused  why do guys do this?'</td>\n",
       "      <td>0</td>\n",
       "      <td>kinda sad and confused why do guys do this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>b'@Real_DavidCook YES &amp;amp; YES '</td>\n",
       "      <td>4</td>\n",
       "      <td>yes and yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>b\"@GDGOfficial But it's another beautiful day ...</td>\n",
       "      <td>4</td>\n",
       "      <td>but its another beautiful day here in europe y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>b'Working through hundreds of assignments '</td>\n",
       "      <td>0</td>\n",
       "      <td>working through hundreds of assignments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>b'driving with the moonroof and windows open i...</td>\n",
       "      <td>0</td>\n",
       "      <td>driving with the moonroof and windows open is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>b\"@scott_mills Gutted! I worked for the fringe...</td>\n",
       "      <td>0</td>\n",
       "      <td>gutted i worked for the fringe last year wont ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Text  Label  \\\n",
       "0           0                b\"i'm 10x cooler than all of you! \"      4   \n",
       "1           1  b'O.kk? Thats weird I cant stop following peop...      0   \n",
       "2           2  b'what a beautiful day not to got to my first ...      4   \n",
       "3           3  b\".@HildyGottlieb &amp; I was just saying to M...      4   \n",
       "4           4    b'kinda sad and confused  why do guys do this?'      0   \n",
       "5           5                  b'@Real_DavidCook YES &amp; YES '      4   \n",
       "6           6  b\"@GDGOfficial But it's another beautiful day ...      4   \n",
       "7           7        b'Working through hundreds of assignments '      0   \n",
       "8           8  b'driving with the moonroof and windows open i...      0   \n",
       "9           9  b\"@scott_mills Gutted! I worked for the fringe...      0   \n",
       "\n",
       "                                           CleanText  \n",
       "0                      im 10x cooler than all of you  \n",
       "1  o kk thats weird i cant stop following people ...  \n",
       "2  what a beautiful day not to got to my first class  \n",
       "3  and i was just saying to mahaal yesterday ever...  \n",
       "4         kinda sad and confused why do guys do this  \n",
       "5                                        yes and yes  \n",
       "6  but its another beautiful day here in europe y...  \n",
       "7            working through hundreds of assignments  \n",
       "8  driving with the moonroof and windows open is ...  \n",
       "9  gutted i worked for the fringe last year wont ...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "df = df.iloc[0:120000, :]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip(\"b'\").strip('b\"')\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = re.sub('@(\\w+)', '', text)\n",
    "    text = re.sub('\\W+', ' ', text)\n",
    "    text = text.replace('amp', 'and').strip()\n",
    "    return text\n",
    "\n",
    "df['CleanText'] = df['Text'].map(clean_text)\n",
    "df.loc[df['Label'] == 0, 'Label'] = 0\n",
    "df.loc[df['Label'] == 4, 'Label'] = 1\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "60gEz8bQ-92u"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9vKWowq_KWN",
    "outputId": "94ed536a-0463-4bfa-b0c3-99221555b72e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    96000\n",
       "Text          96000\n",
       "Label         96000\n",
       "CleanText     96000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gDm3p6V_Mg9",
    "outputId": "4b2bedfb-c00b-46eb-f965-2b37940b7f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    24000\n",
       "Text          24000\n",
       "Label         24000\n",
       "CleanText     24000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPFOspG--gl5",
    "outputId": "6826cac1-893b-4a02-f0ee-66e9e522c53d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    48135\n",
       "0    47865\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExDmHjWF-lVI",
    "outputId": "3e4a6632-6843-4ce5-fc80-93736010917e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    12033\n",
       "0    11967\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPj8aa9Z4ZCg",
    "outputId": "50f14c2c-d38e-4a7a-d5db-fc5919354056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+\n",
      "|Unnamed: 0|                Text|Label|           CleanText|\n",
      "+----------+--------------------+-----+--------------------+\n",
      "|     68758|b'Today is a good...|    4| today is a good day|\n",
      "|     51808|b'@AundreaFimbres...|    4|hi tell your mom ...|\n",
      "|     75098|b'Tweet tweet. Cl...|    4|tweet tweet cla a...|\n",
      "|    102652|b'@ilovecpstyle a...|    4|arizona has a gre...|\n",
      "|     95952|b'@Gatchy Ewwwwww...|    4|ewwwwww and yes y...|\n",
      "|    118442|b\"@alisonswartz I...|    0|im right there wi...|\n",
      "|     98087|b'More shell scri...|    0|more shell scripting|\n",
      "|     77142|b'Finaly got some...|    4|finaly got some d...|\n",
      "|     13722|b'pink lemonade h...|    0|pink lemonade hat...|\n",
      "|     57572|b'@number58 Sadly...|    0|sadly those are o...|\n",
      "+----------+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 04:02:05 WARN org.apache.spark.scheduler.TaskSetManager: Stage 94 contains a task of very large size (7413 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "path_name = 'gs://eecs6893-hw2-cluster/final-project/training/sentiment140.csv'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.executor.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "train_data = spark.createDataFrame(train)\n",
    "test_data = spark.createDataFrame(test)\n",
    "train_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPJRWM-gM7Ht",
    "outputId": "93d8e26e-dd27-498c-9b90-26bba60c316a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: long (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label: long (nullable = true)\n",
      " |-- CleanText: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "L-y6PfJ3Mkjm"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType ,BooleanType, DateType, DoubleType\n",
    "\n",
    "train_data = train_data.withColumn('IntLabel', train_data['Label'].cast(IntegerType()))\n",
    "test_data = test_data.withColumn('IntLabel', test_data['Label'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    " \n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    " \n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(input_col='Text', output_col='features', n=3):\n",
    "        \n",
    "    tokenizer = [Tokenizer(inputCol=input_col, outputCol=\"words\")]\n",
    "    \n",
    "    stop_words = [StopWordsRemover(inputCol='words', outputCol='filterWords')]\n",
    "        \n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"filterWords\", outputCol=\"%s_ngrams\" % str(i))\n",
    "        for i in range(1, n+1)\n",
    "    ]\n",
    " \n",
    "    cv = [CountVectorizer(vocabSize=100e3, inputCol=\"%s_ngrams\" % str(i),\n",
    "            outputCol=\"%s_tf\" % str(i))\n",
    "        for i in range(1, n+1)]\n",
    "    \n",
    "    idf = [IDF(inputCol=\"%s_tf\" % str(i), outputCol=\"%s_tfidf\" % str(i), minDocFreq=100) for i in range(1, n+1)]\n",
    " \n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"%s_tfidf\" % str(i) for i in range(1, n+1)],\n",
    "        outputCol=output_col)]\n",
    "    \n",
    "    stages = tokenizer + stop_words + ngrams + cv + idf + assembler\n",
    "    \n",
    "    return stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_model(model_type, label_col='IntLabel', features_col='features'):\n",
    "    \n",
    "    if model_type == 'logistic':\n",
    "        model = LogisticRegression(labelCol=label_col, featuresCol=features_col, maxIter=10, regParam=0.01, elasticNetParam=0.8)\n",
    "    if model_type == 'gbt':\n",
    "        model = GBTClassifier(labelCol=label_col, featuresCol=features_col, maxIter=10)\n",
    "    if model_type == 'svm':\n",
    "        model = LinearSVC(labelCol=label_col, featuresCol=features_col, maxIter=10, regParam=0.01)\n",
    "    if model_type == 'nb':\n",
    "        model = NaiveBayes(labelCol=label_col, featuresCol=features_col, smoothing=1.0)\n",
    "    \n",
    "    label_index = StringIndexer(inputCol=\"Label\", outputCol=\"StringLabel\")\n",
    "    stages = [model]\n",
    "    \n",
    "    return stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_model(train_data, model_type, n):\n",
    "    \n",
    "    feature_pipeline = get_ngrams(n=n)\n",
    "    model_pipeline = get_ml_model(model_type)\n",
    "    total_pipeline = Pipeline(stages=feature_pipeline + model_pipeline)\n",
    "    total_pipeline = total_pipeline.fit(train_data)\n",
    "    \n",
    "    return total_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline_model(model, test_data):\n",
    "\n",
    "    predictions = model.transform(test_data)\n",
    "    preds = predictions.select('IntLabel', \"prediction\")\n",
    "    preds = preds.withColumn('PredLabel', preds['prediction'].cast(DoubleType()))\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"IntLabel\", predictionCol=\"PredLabel\", metricName=\"accuracy\")\n",
    "    test_acc = evaluator.evaluate(preds)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 19:11:25 WARN org.apache.spark.scheduler.TaskSetManager: Stage 278 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:30 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1253.6 KiB\n",
      "21/11/18 19:11:30 WARN org.apache.spark.scheduler.TaskSetManager: Stage 282 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:41 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:41 WARN org.apache.spark.scheduler.TaskSetManager: Stage 283 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:45 WARN org.apache.spark.scheduler.TaskSetManager: Stage 284 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:48 WARN org.apache.spark.scheduler.TaskSetManager: Stage 285 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:49 WARN org.apache.spark.scheduler.TaskSetManager: Stage 286 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:50 WARN org.apache.spark.scheduler.TaskSetManager: Stage 287 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:50 WARN org.apache.spark.scheduler.TaskSetManager: Stage 288 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:51 WARN org.apache.spark.scheduler.TaskSetManager: Stage 289 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:51 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:51 WARN org.apache.spark.scheduler.TaskSetManager: Stage 290 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:52 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:52 WARN org.apache.spark.scheduler.TaskSetManager: Stage 291 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:53 WARN org.apache.spark.scheduler.TaskSetManager: Stage 292 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:53 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:53 WARN org.apache.spark.scheduler.TaskSetManager: Stage 293 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:54 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:54 WARN org.apache.spark.scheduler.TaskSetManager: Stage 294 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:55 WARN org.apache.spark.scheduler.TaskSetManager: Stage 295 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:55 WARN org.apache.spark.scheduler.TaskSetManager: Stage 296 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:56 WARN org.apache.spark.scheduler.TaskSetManager: Stage 297 contains a task of very large size (6141 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:11:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:11:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 298 contains a task of very large size (6130 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:12:05 WARN org.apache.spark.scheduler.TaskSetManager: Stage 304 contains a task of very large size (2027 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/18 19:12:07 WARN org.apache.spark.scheduler.TaskSetManager: Stage 308 contains a task of very large size (1599 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC-NGRAM=1-2021-11-18-19-11-57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 19:12:13 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 8.2 MiB\n",
      "21/11/18 19:12:13 WARN org.apache.spark.scheduler.TaskSetManager: Stage 316 contains a task of very large size (1578 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 316:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy = 0.68305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "baseline_models = ['logistic', 'gbt', 'svm']\n",
    "n_grams = [1]\n",
    "results = []\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "for baseline_model in baseline_models:\n",
    "    \n",
    "    for n_gram in n_grams:\n",
    "    \n",
    "        model = train_baseline_model(train_data, baseline_model, n_gram)\n",
    "        t1 = time.time()\n",
    "        ts = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        path_name = 'gs://eecs6893-hw2-cluster/final-project/training/'\n",
    "        model_name = baseline_model.upper() + '-NGRAM=' + str(n_gram) + '-' + ts\n",
    "        model.save(path_name + model_name)\n",
    "        print(model_name)\n",
    "        t2 = time.time()\n",
    "        te = (t2-t1)\n",
    "        test_acc = test_baseline_model(model, test_data)\n",
    "        result = (baseline_model.upper(), n_gram, te, test_acc)\n",
    "        results.append(result)\n",
    "        print(\"Test Set Accuracy = \" + str(test_acc))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Ngrams', 'Train Time', 'Test Accuracy'])\n",
    "results_df.to_csv(path_name + 'perf_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Ngrams</th>\n",
       "      <th>Train Time</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOGISTIC</td>\n",
       "      <td>1</td>\n",
       "      <td>15.276312</td>\n",
       "      <td>0.68305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Ngrams  Train Time  Test Accuracy\n",
       "0  LOGISTIC       1   15.276312        0.68305"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "      <td>-0.433188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miss</td>\n",
       "      <td>-0.351202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>-0.257663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sorry</td>\n",
       "      <td>-0.244846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wish</td>\n",
       "      <td>-0.240993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>poor</td>\n",
       "      <td>-0.234136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sick</td>\n",
       "      <td>-0.226312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sucks</td>\n",
       "      <td>-0.211375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hurts</td>\n",
       "      <td>-0.196153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lost</td>\n",
       "      <td>-0.189007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word    Weight\n",
       "0    sad -0.433188\n",
       "1   miss -0.351202\n",
       "2   hate -0.257663\n",
       "3  sorry -0.244846\n",
       "4   wish -0.240993\n",
       "5   poor -0.234136\n",
       "6   sick -0.226312\n",
       "7  sucks -0.211375\n",
       "8  hurts -0.196153\n",
       "9   lost -0.189007"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.stages[-1].coefficients.toArray()\n",
    "vocab = np.array(model.stages[3].vocabulary)\n",
    "negative_indices = np.argsort(weights)[:10]\n",
    "negative_weights = np.sort(weights)[:10]\n",
    "negative_words = vocab[negative_indices]\n",
    "df1 = pd.DataFrame(zip(negative_words, negative_weights), columns=['Word', 'Weight'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>0.321913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'</td>\n",
       "      <td>0.304056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank</td>\n",
       "      <td>0.242933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>welcome</td>\n",
       "      <td>0.191254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>0.190473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>great</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.169721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>awesome</td>\n",
       "      <td>0.155250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nice</td>\n",
       "      <td>0.135478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>glad</td>\n",
       "      <td>0.124331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thanks!</td>\n",
       "      <td>0.105634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word    Weight\n",
       "0    thanks  0.321913\n",
       "1         '  0.304056\n",
       "2      love  0.250400\n",
       "3     thank  0.242933\n",
       "4   welcome  0.191254\n",
       "5      good  0.190473\n",
       "6     great  0.185500\n",
       "7     happy  0.169721\n",
       "8   awesome  0.155250\n",
       "9      nice  0.135478\n",
       "10     glad  0.124331\n",
       "11  thanks!  0.105634"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_indices = np.argsort(weights)[-12:]\n",
    "negative_weights = np.sort(weights)[-12:][::-1]\n",
    "negative_words = vocab[negative_indices][::-1]\n",
    "df2 = pd.DataFrame(zip(negative_words, negative_weights), columns=['Word', 'Weight'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cyp_hsF_4ZCh"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(train_data, embedding_model, batch_size, num_epochs, learning_rate, dropout_rate=0.00, spell_check=False):\n",
    "\n",
    "    documentAssembler = DocumentAssembler().setInputCol(\"CleanText\").setOutputCol(\"document\")\n",
    "    sentence = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "    if spell_check:\n",
    "        tokenizer = RecursiveTokenizer() \\\n",
    "                    .setInputCols([\"document\"]) \\\n",
    "                    .setOutputCol(\"token\") \\\n",
    "                    .setPrefixes([\"\\\"\", \"“\", \"(\", \"[\", \"\\n\", \".\"]) \\\n",
    "                    .setSuffixes([\"\\\"\", \"”\", \".\", \",\", \"?\", \")\", \"]\", \"!\", \";\", \":\", \"'s\", \"’s\"])\n",
    "\n",
    "        spellModel = ContextSpellCheckerModel \\\n",
    "                    .pretrained() \\\n",
    "                    .setInputCols(\"token\") \\\n",
    "                    .setOutputCol(\"checked\") \\\n",
    "    else:\n",
    "        tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "\n",
    "    if embedding_model == 'roberta':\n",
    "        embeddings = RoBertaSentenceEmbeddings.pretrained()\n",
    "    elif embedding_model == 'xlm_roberta':\n",
    "        embeddings = XlmRoBertaSentenceEmbeddings.pretrained()\n",
    "    elif embedding_model == 'bert':\n",
    "        embeddings = BertSentenceEmbeddings.pretrained()\n",
    "    elif embedding_model == 'elmo':\n",
    "        embeddings = ElmoEmbeddings.pretrained()\n",
    "    elif embedding_model == 'use':\n",
    "        embeddings = UniversalSentenceEncoder.pretrained()\n",
    "    elif embedding_model == 'distilbert':\n",
    "        embeddings = DistilBertEmbeddings.pretrained()\n",
    "\n",
    "    embeddings = embeddings.setInputCols([\"document\"]).setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "    classifier = ClassifierDLApproach() \\\n",
    "      .setInputCols(\"sentence_embeddings\") \\\n",
    "      .setOutputCol(\"prediction\") \\\n",
    "      .setLabelColumn(\"IntLabel\") \\\n",
    "      .setBatchSize(batch_size) \\\n",
    "      .setMaxEpochs(num_epochs) \\\n",
    "      .setLr(learning_rate) \\\n",
    "      .setDropout(dropout_rate) \\\n",
    "      .setVerbose(0) \\\n",
    "      .setEnableOutputLogs(True)\n",
    "\n",
    "    model_pipeline = Pipeline().setStages([\n",
    "      documentAssembler,\n",
    "      embeddings,\n",
    "      classifier\n",
    "    ])\n",
    "\n",
    "    model = model_pipeline.fit(train_data)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "suic24XaSEgl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE\n",
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 02:47:55.827480: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-19 02:47:56.184702: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/19 02:48:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 2 contains a task of very large size (7413 KiB). The maximum recommended task size is 1000 KiB.\n",
      "2021-11-19 02:48:22.028564: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: /tmp/dca1aafe3304_classifier_dl2172160671251170991\n",
      "2021-11-19 02:48:22.118688: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\n",
      "2021-11-19 02:48:22.118757: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: /tmp/dca1aafe3304_classifier_dl2172160671251170991\n",
      "2021-11-19 02:48:22.633352: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2021-11-19 02:48:23.646846: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /tmp/dca1aafe3304_classifier_dl2172160671251170991\n",
      "2021-11-19 02:48:23.812862: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1784962 microseconds.\n",
      "21/11/19 02:48:25 WARN org.apache.spark.scheduler.TaskSetManager: Stage 5 contains a task of very large size (7413 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/11/19 02:51:36 WARN org.apache.spark.HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 167816 ms exceeds timeout 120000 ms\n",
      "21/11/19 02:51:36 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on final-project2-w-0.us-east1-d.c.eecs6893-325818.internal: Executor heartbeat timed out after 167816 ms\n",
      "21/11/19 02:51:36 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 6) (final-project2-w-0.us-east1-d.c.eecs6893-325818.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167816 ms\n",
      "21/11/19 02:51:54 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1637258899703_0004_01_000003 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:51:54.833]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:51:54.833]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:51:54.833]Killed by external signal\n",
      ".\n",
      "21/11/19 02:51:54 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 3 on final-project2-w-1.us-east1-d.c.eecs6893-325818.internal: Container from a bad node: container_1637258899703_0004_01_000003 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:51:54.833]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:51:54.833]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:51:54.833]Killed by external signal\n",
      ".\n",
      "21/11/19 02:51:54 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.1 in stage 5.0 (TID 7) (final-project2-w-1.us-east1-d.c.eecs6893-325818.internal executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637258899703_0004_01_000003 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:51:54.833]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:51:54.833]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:51:54.833]Killed by external signal\n",
      ".\n",
      "21/11/19 02:51:54 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1637258899703_0004_01_000003 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:51:54.833]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:51:54.833]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:51:54.833]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:15 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1637258899703_0004_01_000004 on host: final-project2-w-0.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:15.474]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:15.474]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:15.475]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:15 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 4 on final-project2-w-0.us-east1-d.c.eecs6893-325818.internal: Container from a bad node: container_1637258899703_0004_01_000004 on host: final-project2-w-0.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:15.474]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:15.474]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:15.475]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:15 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1637258899703_0004_01_000004 on host: final-project2-w-0.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:15.474]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:15.474]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:15.475]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:15 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.2 in stage 5.0 (TID 8) (final-project2-w-0.us-east1-d.c.eecs6893-325818.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637258899703_0004_01_000004 on host: final-project2-w-0.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:15.474]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:15.474]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:15.475]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:34 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1637258899703_0004_01_000005 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:34.076]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:34.076]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:34.079]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:34 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1637258899703_0004_01_000005 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:34.076]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:34.076]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:34.079]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:34 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 5 on final-project2-w-1.us-east1-d.c.eecs6893-325818.internal: Container from a bad node: container_1637258899703_0004_01_000005 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:34.076]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:34.076]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:34.079]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:34 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.3 in stage 5.0 (TID 9) (final-project2-w-1.us-east1-d.c.eecs6893-325818.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637258899703_0004_01_000005 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:34.076]Container killed on request. Exit code is 143\n",
      "[2021-11-19 02:52:34.076]Container exited with a non-zero exit code 143. \n",
      "[2021-11-19 02:52:34.079]Killed by external signal\n",
      ".\n",
      "21/11/19 02:52:34 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o227.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 9) (final-project2-w-1.us-east1-d.c.eecs6893-325818.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637258899703_0004_01_000005 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:34.076]Container killed on request. Exit code is 143\n[2021-11-19 02:52:34.076]Container exited with a non-zero exit code 143. \n[2021-11-19 02:52:34.079]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n\tat com.johnsnowlabs.ml.tensorflow.ClassifierDatasetEncoder.calculateEmbeddingsDim(ClassifierDatasetEncoder.scala:186)\n\tat com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach.train(ClassifierDLApproach.scala:368)\n\tat com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach.train(ClassifierDLApproach.scala:114)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:68)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:74)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3720/1146427325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d-%H-%M-%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpath_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gs://eecs6893-hw2-cluster/final-project/training/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3720/674295208.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data, embedding_model, batch_size, num_epochs, learning_rate, dropout_rate, spell_check)\u001b[0m\n\u001b[1;32m     59\u001b[0m     ])\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o227.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 9) (final-project2-w-1.us-east1-d.c.eecs6893-325818.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637258899703_0004_01_000005 on host: final-project2-w-1.us-east1-d.c.eecs6893-325818.internal. Exit status: 143. Diagnostics: [2021-11-19 02:52:34.076]Container killed on request. Exit code is 143\n[2021-11-19 02:52:34.076]Container exited with a non-zero exit code 143. \n[2021-11-19 02:52:34.079]Killed by external signal\n.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2204)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2225)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2244)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n\tat com.johnsnowlabs.ml.tensorflow.ClassifierDatasetEncoder.calculateEmbeddingsDim(ClassifierDatasetEncoder.scala:186)\n\tat com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach.train(ClassifierDLApproach.scala:368)\n\tat com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach.train(ClassifierDLApproach.scala:114)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:68)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:74)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.classification as mlc\n",
    "import time\n",
    "import datetime as dt\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "embedding_models = ['bert', 'roberta', 'use']\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "learning_rate = 5e-4\n",
    "dropout_rate = 0.50\n",
    "results = []\n",
    "\n",
    "for embedding_model in embedding_models:\n",
    "    print(embedding_model.upper())\n",
    "    t1 = time.time()\n",
    "    model = train_model(train_data, embedding_model, batch_size, num_epochs, learning_rate, dropout_rate=0.00)\n",
    "    ts = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    path_name = 'gs://eecs6893-hw2-cluster/final-project/training/'\n",
    "    model_name = embedding_model.upper() + '-' + str(batch_size) + '-' + str(num_epochs) + '-' + ts\n",
    "    print(model_name)\n",
    "    model.save(path_name + model_name)\n",
    "    t2 = time.time()\n",
    "    te = (t2-t1)\n",
    "    predictions = model.transform(test_data)\n",
    "    preds = predictions.select('IntLabel', \"prediction.result\")\n",
    "    preds = preds.withColumn('PredLabel', preds['result'].getItem(0).cast(DoubleType()))\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"IntLabel\", predictionCol=\"PredLabel\", metricName=\"accuracy\")\n",
    "    test_acc = evaluator.evaluate(preds)\n",
    "    result = (embedding_model, te, test_acc)\n",
    "    results.append(result)\n",
    "    print(\"Test Set Accuracy = \" + str(test_acc))\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Embedding', 'Train Time', 'Test Accuracy'])\n",
    "results_df.head(12)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebooks_jupyter_FinalProject_ModelTraining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
